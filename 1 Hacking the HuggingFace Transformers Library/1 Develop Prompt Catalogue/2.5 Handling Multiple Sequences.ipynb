{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Write code using Transformers to process batches with multiple text sequences efficiently.â€‹"
      ],
      "metadata": {
        "id": "Up5pY-t6Tneh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhqqD9INQF05"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Define the model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # You can change this to any other pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Example input texts\n",
        "texts = [\"This is the first sequence.\", \"This is the second sequence.\", \"And this is the third sequence.\"]\n",
        "\n",
        "# Tokenize the input texts\n",
        "tokenized_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "# Forward pass through the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized_inputs)\n",
        "\n",
        "# Extract the output embeddings or logits\n",
        "output_logits = outputs.logits  # For classification models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can now use the output_embeddings or output_logits as needed for your task.\n",
        "# If you are doing sequence classification, you might want to apply a softmax to the logits.\n",
        "output_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9O1sGCsSBHs",
        "outputId": "d2fb9c07-b0e7-4490-edb7-80bd468c29b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3130,  0.7362],\n",
              "        [-0.3060,  0.6934],\n",
              "        [-0.3489,  0.6343]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}